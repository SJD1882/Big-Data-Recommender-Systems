{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUcRu2HYkA0DGIveuAPwe5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SJD1882/Big-Data-Recommender-Systems/blob/master/CNB01_AutoGPTQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Open Source Large Language Models**\n",
        "\n",
        "# **Colab: *Using AutoGPTQ for loading LLM into HuggingFace Transformers and Langchain***"
      ],
      "metadata": {
        "id": "fTthaP-RAnMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setings**"
      ],
      "metadata": {
        "id": "qLqJjPnWB_Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "is_show_reply = True\n",
        "LLM_MAX_TOKENS = 2042\n",
        "LLM_TOP_P = 0.9\n",
        "LLM_TOP_K = 150\n",
        "LLM_REPETITION_PENALTY = 1.1\n",
        "LLM_TEMPERATURE = 0.1\n",
        "LLM_HF_MODEL_REPOSITORY = 'TheBloke/Manticore-13B-GPTQ'"
      ],
      "metadata": {
        "id": "GxkcyfIrCAxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**"
      ],
      "metadata": {
        "id": "KdLjgKy-BD1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4-bit integer post-training quantization has been successfully used to reduce the massive GPU VRAM requirements of Large Language Models (*Done*)\n",
        "- As of May 21nd 2023, provide a working example of how to use AutoGPTQ to load HuggingFace GPTQ models in Transformers and Langchain (*Done*)\n",
        "- Run custom prompts to check if LLM works (*Done*)\n",
        "- Run a more quantitative benchmark like TruthfulQA (*Ongoing*)\n",
        "- Store results as a Parquet file for later MLFlow Tracking (*Ongoing*)"
      ],
      "metadata": {
        "id": "apHhIgNqBHde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**"
      ],
      "metadata": {
        "id": "0kG_YxkOBGHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Runtime environment must include an Nvidia GPU (either T4 15GB VRAM, V100 16GB or A100 40GB).\n",
        "- Google Colab Pro is recommended. But bven then, I still recommend that you avoid using an A100 GPU (and only as a last resort if even the quantized LLM versions are computationally way to expensive). Your Colab compute units will burn fast with an A100: at least 10 per hour, whereas starting with a V100 15GB VRAM will only use up to 5 compute units and less powerful but equally stocked T4 15GB VRAM will only cost you 2 compute units."
      ],
      "metadata": {
        "id": "-JSI_EX1BIHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**"
      ],
      "metadata": {
        "id": "DTnmpRYtBPpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Regularly updated quantized versions of LLMs on HuggingFace by user TheBloke](https://huggingface.co/TheBloke)\n",
        "- [Analysis of VRAM requirements and performance of finetuned LLaMA models on r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/wiki/models/)\n",
        "- [GitHub Repository for AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)"
      ],
      "metadata": {
        "id": "oSiewh4xBbwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Ressources**"
      ],
      "metadata": {
        "id": "X0n5ZEZCC5JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"CPU RAM\"\n",
        "!free -gh\n",
        "!echo \"\"\n",
        "!echo \"\"\n",
        "!echo \"GPU VRAM\"\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaDofjSSBQTn",
        "outputId": "060b2283-24be-482f-e4e5-1639489a92a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU RAM\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:           50Gi       679Mi        46Gi       1.0Mi       3.6Gi        49Gi\n",
            "Swap:            0B          0B          0B\n",
            "\n",
            "\n",
            "GPU VRAM\n",
            "Sun May 21 15:09:07 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Setup**"
      ],
      "metadata": {
        "id": "LVAAP1EwDCCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Installation"
      ],
      "metadata": {
        "id": "Fsdv8uKZEGGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "mkdir -p /content/results/"
      ],
      "metadata": {
        "id": "doBBPZF3D3-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "pip -q install git+https://github.com/huggingface/transformers\n",
        "pip install -q datasets loralib sentencepiece accelerate\n",
        "pip install -q bitsandbytes langchain xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgu9v4snDCZZ",
        "outputId": "2a8ac6c6-b0c2-428d-f1f7-cc893d1e11fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 18.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 101.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 26.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 5.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 219.1/219.1 kB 20.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 14.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 24.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 17.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 60.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 14.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 22.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 18.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.3/104.3 MB 18.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 872.8/872.8 kB 68.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.2/108.2 MB 16.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.0/90.0 kB 11.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 2.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 84.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 63.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 102.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 2.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 2.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 10.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 MB 33.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 MB 18.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 MB 4.0 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.1/177.1 MB 7.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 kB 12.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.1/49.1 kB 6.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "mkdir -p /content/repos/\n",
        "if [ -d '/content/repos/AutoGPTQ/' ]; then\n",
        "    echo \"Repository already downloaded.\"\n",
        "else\n",
        "    echo \"Downloading AutoGPTQ.\"\n",
        "    git clone https://github.com/PanQiWei/AutoGPTQ.git /content/repos/AutoGPTQ/\n",
        "    cd /content/repos/AutoGPTQ/\n",
        "    pip install .[llama]\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4U0VNQED1kM",
        "outputId": "a65ddf22-b888-4357-e328-47754d030781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading AutoGPTQ.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/repos/AutoGPTQ\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: accelerate>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.2.0.dev0) (0.19.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.2.0.dev0) (2.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.2.0.dev0) (1.22.4)\n",
            "Collecting rouge (from auto-gptq==0.2.0.dev0)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.2.0.dev0) (2.0.0)\n",
            "Collecting safetensors (from auto-gptq==0.2.0.dev0)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 2.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.2.0.dev0) (4.30.0.dev0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.18.0->auto-gptq==0.2.0.dev0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.18.0->auto-gptq==0.2.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.18.0->auto-gptq==0.2.0.dev0) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.2.0.dev0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->auto-gptq==0.2.0.dev0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->auto-gptq==0.2.0.dev0) (0.40.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq==0.2.0.dev0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq==0.2.0.dev0) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->auto-gptq==0.2.0.dev0) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->auto-gptq==0.2.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->auto-gptq==0.2.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->auto-gptq==0.2.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->auto-gptq==0.2.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.0.dev0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.0.dev0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.0.dev0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.0.dev0) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.0.dev0) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.0.dev0) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.2.0.dev0) (0.18.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq==0.2.0.dev0) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.2.0.dev0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.2.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.2.0.dev0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.2.0.dev0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.2.0.dev0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.2.0.dev0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.2.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->auto-gptq==0.2.0.dev0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq==0.2.0.dev0) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq==0.2.0.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq==0.2.0.dev0) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq==0.2.0.dev0) (1.3.0)\n",
            "Building wheels for collected packages: auto-gptq\n",
            "  Building wheel for auto-gptq (setup.py): started\n",
            "  Building wheel for auto-gptq (setup.py): finished with status 'done'\n",
            "  Created wheel for auto-gptq: filename=auto_gptq-0.2.0.dev0-cp310-cp310-linux_x86_64.whl size=3642340 sha256=b4529f2933550376c5d29491da53fa34e2de0363f333c75e968e0cb81718a405\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cdogd8rc/wheels/20/da/86/9c344eaca36b7fcc6fbee9a445f0f970e2e6c13a8094d723ac\n",
            "Successfully built auto-gptq\n",
            "Installing collected packages: safetensors, rouge, auto-gptq\n",
            "Successfully installed auto-gptq-0.2.0.dev0 rouge-1.0.1 safetensors-0.3.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into '/content/repos/AutoGPTQ'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Packages"
      ],
      "metadata": {
        "id": "2TbiVHaOEIFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import glob\n",
        "import logging\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, TextGenerationPipeline, \\\n",
        "                         pipeline, set_seed\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from datasets import load_dataset\n",
        "from IPython.display import display, HTML\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "\n",
        "# Set random states\n",
        "set_seed(SEED)\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Silence warnings from HF Transformers\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT1JERszEIUV",
        "outputId": "4d7bf0c6-c910-43f4-d7b4-fd959071825b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.utilities.powerbi:Could not import azure.core python package.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Utils"
      ],
      "metadata": {
        "id": "YS9S9yK2EKOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(llm_chain, prompt):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "        result = llm_chain.predict(prompt=prompt)\n",
        "        result = re.sub(r'\\S{51,}', '', result).strip()\n",
        "    return result\n",
        "\n",
        "def print_llm_output(llm_chain, prompt, result, width=80):\n",
        "    start = llm_chain.prompt.template.format(prompt=prompt)\n",
        "    result = f'<br><b>{result.lstrip()}</b>'\n",
        "    start_result = start + result\n",
        "    start_result = start_result.replace('\\n', '<br>')\n",
        "    start_result += '<END>'\n",
        "    display(HTML(start_result))\n",
        "\n",
        "def get_response_with_output(llm_chain, prompt, width=80,\n",
        "                             is_show_result=False):\n",
        "    answer = get_response(llm_chain, prompt)\n",
        "    if is_show_result:\n",
        "        print_llm_output(llm_chain, prompt, answer, width=width)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "2b4BKZooEKc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4. Load evaluation dataset"
      ],
      "metadata": {
        "id": "GtQWiR7-H5sE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add a bunch of custom questions**"
      ],
      "metadata": {
        "id": "kw9q0zl7N0DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_questions = [\n",
        "\n",
        "    ['general_qa', 'Give me the capital of Belize'],\n",
        "    ['general_qa', 'Give me the capital cities of the following countries: France, United States, Germany, Russia, Ukraine, Estonia, Uzbekistan, Brunei, Rwanda, South Sudan, Paraguay'],\n",
        "    ['general_qa', 'Give me the state capitols of the following countries: Kentucky, South Dakota, Vermont, Delaware, California, Utah, Louisiana, Alaska and Idaho'],\n",
        "    ['general_qa', 'Give the voice actress of Female Commender Shepard in the video game series Mass Effect'],\n",
        "    ['general_qa', 'Who won the Oscar for Best Picture in 1941?'],\n",
        "    ['general_qa', 'Give me the winner of each French presidential election under the French Vth Republic'],\n",
        "    ['general_qa', \"Give me the definition of the VC Dimension and provide if possible some mathematical notations\"],\n",
        "    ['general_qa', 'Give me the 12 NPC companions available to Commander Shepard in Mass Effect 2'],\n",
        "    ['general_qa', 'Give me a couple of Moldavian Heavy Metal Bands'],\n",
        "    ['general_qa', 'Give me five movies directed by the Coen Brothers before 2000 and sorted by year of release'],\n",
        "    ['general_qa', 'Alexander the Great defeated Darius III at the Battle of Gaugamela. Give me details on the military tactics Alexander used to defeat Darius III.'],\n",
        "    ['general_qa', 'Tell me who was the more impressive historical figure: Alexander the Great or Genghis Khan?'],\n",
        "    ['general_qa', 'What is the Metal subgenre of the band Meshuggah?'],\n",
        "    ['general_qa', \"Could you give me the 4 main factions of Fallout New Vegas' main story quest?\"],\n",
        "    ['general_qa', \"Describe the Mormons' views on the Holy Trinity\"],\n",
        "    ['general_qa', \"Ecris-moi une dissertation sur le sujet de philosophie: la vie est-elle autre chose que le théâtre de la cruauté ?\"],\n",
        "\n",
        "    ['essay_writing', 'Write me an essay in which you argue that Internet filter bubbles are good for society. Give me examples illustrating your main point.'],\n",
        "    ['essay_writing', 'Write me an essay where you argue that being exposed to opposing political viewpoints is counter-productive.'],\n",
        "    ['essay_writing', 'Write me an essay answering the following philosophical question: \"Is life nothing more than a cruel theater play?\"'],\n",
        "    ['essay_writing', 'Write an essay in support of deploying lethal autonomous military robots in modern warfare.'],\n",
        "\n",
        "    ['creative_writing', 'Write a cover letter for a Data Scientist requiring 3 years of experience, skills in AWS, Pandas Python and Scikit-Learn'],\n",
        "    ['creative_writing', 'Write a gritty war scene set in Omaha Beach on June 6th 1944 during D-Day from the perspective of an American soldier. Add dialogue to the scene.'],\n",
        "    ['creative_writing', 'Write a short story set in New York where a thermonuclear bomb annihilates the city.'],\n",
        "    ['creative_writing', 'Write a short story set in New York where a thermonuclear bomb annihilates the city in the style of Chuck Palahniuk.'],\n",
        "\n",
        "    ['code_generation', 'Write a Python code that generates the column schema of Excel, example: \"A, B, ... Z, AA, AB, AC, ... \"'],\n",
        "    ['code_generation', 'Generate me Python code for training a Linear Regression on the Boston housing prices dataset'],\n",
        "    ['code_generation', 'Give me a Regex code for extracting the year in the following string: \"The Downward Spiral, Nine Inch Nails (1994)\"']\n",
        "]\n",
        "\n",
        "custom_questions_df = pd.DataFrame(\n",
        "    custom_questions,\n",
        "    columns=['category', 'instruction']\n",
        ")\n",
        "\n",
        "custom_questions_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "xLZn7Origohh",
        "outputId": "87fa9199-da5a-47bb-eaf8-201ecf521fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            category                                        instruction\n",
              "0         general_qa                      Give me the capital of Belize\n",
              "1         general_qa  Give me the capital cities of the following co...\n",
              "2         general_qa  Give me the state capitols of the following co...\n",
              "3         general_qa  Give the voice actress of Female Commender She...\n",
              "4         general_qa        Who won the Oscar for Best Picture in 1941?\n",
              "5         general_qa  Give me the winner of each French presidential...\n",
              "6         general_qa  Give me the definition of the VC Dimension and...\n",
              "7         general_qa  Give me the 12 NPC companions available to Com...\n",
              "8         general_qa    Give me a couple of Moldavian Heavy Metal Bands\n",
              "9         general_qa  Give me five movies directed by the Coen Broth...\n",
              "10        general_qa  Alexander the Great defeated Darius III at the...\n",
              "11        general_qa  Tell me who was the more impressive historical...\n",
              "12        general_qa  What is the Metal subgenre of the band Meshuggah?\n",
              "13        general_qa  Could you give me the 4 main factions of Fallo...\n",
              "14        general_qa    Describe the Mormons' views on the Holy Trinity\n",
              "15        general_qa  Ecris-moi une dissertation sur le sujet de phi...\n",
              "16     essay_writing  Write me an essay in which you argue that Inte...\n",
              "17     essay_writing  Write me an essay where you argue that being e...\n",
              "18     essay_writing  Write me an essay answering the following phil...\n",
              "19     essay_writing  Write an essay in support of deploying lethal ...\n",
              "20  creative_writing  Write a cover letter for a Data Scientist requ...\n",
              "21  creative_writing  Write a gritty war scene set in Omaha Beach on...\n",
              "22  creative_writing  Write a short story set in New York where a th...\n",
              "23  creative_writing  Write a short story set in New York where a th...\n",
              "24   code_generation  Write a Python code that generates the column ...\n",
              "25   code_generation  Generate me Python code for training a Linear ...\n",
              "26   code_generation  Give me a Regex code for extracting the year i..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-842b40bb-c7ac-4289-b402-90316bf15e0f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>instruction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give me the capital of Belize</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give me the capital cities of the following co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give me the state capitols of the following co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give the voice actress of Female Commender She...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Who won the Oscar for Best Picture in 1941?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give me the winner of each French presidential...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give me the definition of the VC Dimension and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give me the 12 NPC companions available to Com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give me a couple of Moldavian Heavy Metal Bands</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Give me five movies directed by the Coen Broth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Alexander the Great defeated Darius III at the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Tell me who was the more impressive historical...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>What is the Metal subgenre of the band Meshuggah?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Could you give me the 4 main factions of Fallo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Describe the Mormons' views on the Holy Trinity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>general_qa</td>\n",
              "      <td>Ecris-moi une dissertation sur le sujet de phi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>essay_writing</td>\n",
              "      <td>Write me an essay in which you argue that Inte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>essay_writing</td>\n",
              "      <td>Write me an essay where you argue that being e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>essay_writing</td>\n",
              "      <td>Write me an essay answering the following phil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>essay_writing</td>\n",
              "      <td>Write an essay in support of deploying lethal ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>creative_writing</td>\n",
              "      <td>Write a cover letter for a Data Scientist requ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>creative_writing</td>\n",
              "      <td>Write a gritty war scene set in Omaha Beach on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>creative_writing</td>\n",
              "      <td>Write a short story set in New York where a th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>creative_writing</td>\n",
              "      <td>Write a short story set in New York where a th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>code_generation</td>\n",
              "      <td>Write a Python code that generates the column ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>code_generation</td>\n",
              "      <td>Generate me Python code for training a Linear ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>code_generation</td>\n",
              "      <td>Give me a Regex code for extracting the year i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-842b40bb-c7ac-4289-b402-90316bf15e0f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-842b40bb-c7ac-4289-b402-90316bf15e0f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-842b40bb-c7ac-4289-b402-90316bf15e0f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Download and Load Open Source LLM into Langchain**"
      ],
      "metadata": {
        "id": "BaDFpskgDwC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Hyperparameters"
      ],
      "metadata": {
        "id": "cwCA-rzvEXNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_datetime_original = datetime.today()\n",
        "run_datetime = run_datetime_original.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "llm_hyperparameters = {\n",
        "    'RUN_START': run_datetime,\n",
        "    'LLM_MODEL': LLM_HF_MODEL_REPOSITORY,\n",
        "    'LLM_MAX_TOKENS': LLM_MAX_TOKENS,\n",
        "    'LLM_TOP_P': LLM_TOP_P,\n",
        "    'LLM_TOP_K': LLM_TOP_K,\n",
        "    'LLM_REPETITION_PENALTY': LLM_REPETITION_PENALTY,\n",
        "    'LLM_TEMPERATURE': LLM_TEMPERATURE,\n",
        "    'SEED': SEED\n",
        "}\n",
        "\n",
        "pprint(llm_hyperparameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MQQWxlZDxhM",
        "outputId": "e10609f3-79bd-4796-8a45-f6f894496eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'LLM_MAX_TOKENS': 2042,\n",
            " 'LLM_MODEL': 'TheBloke/Manticore-13B-GPTQ',\n",
            " 'LLM_REPETITION_PENALTY': 1.1,\n",
            " 'LLM_TEMPERATURE': 0.1,\n",
            " 'LLM_TOP_K': 150,\n",
            " 'LLM_TOP_P': 0.9,\n",
            " 'RUN_START': '2023-05-21 15:13:50',\n",
            " 'SEED': 42}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. GPTQ"
      ],
      "metadata": {
        "id": "xB6NPnf0EZFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download HuggingFace model from model repository. Ignore \"Encountered 1 file(s) that may not have been copied correctly on Windows\"**. [Source on HuggingFace Discussions of Stable Vicuna 13B GGML by TheBloke](https://huggingface.co/TheBloke/stable-vicuna-13B-GGML/discussions/2)"
      ],
      "metadata": {
        "id": "ySjoG6zpT28G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_HF_MODEL_NAME = LLM_HF_MODEL_REPOSITORY.split('/')[-1]\n",
        "!git lfs install\n",
        "!mkdir -p /content/llm_models/\n",
        "!git clone https://huggingface.co/$LLM_HF_MODEL_REPOSITORY /content/llm_models/$LLM_HF_MODEL_NAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUrdWfI5EZUp",
        "outputId": "59c53964-098b-48c2-b7e4-9e811b49f903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into '/content/llm_models/Manticore-13B-GPTQ'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 25 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), 481.74 KiB | 1.87 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 2.94 GiB | 41.07 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tManticore-13B-GPTQ-4bit-128g.no-act-order.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ignore the \"The model 'LlamaGPTQForCausalLM' is not supported for text-generation\"**"
      ],
      "metadata": {
        "id": "W37PCFmrTyuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Settings\n",
        "llm_hf_folder = f'/content/llm_models/{LLM_HF_MODEL_NAME}/'\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(llm_hf_folder, use_fast=True)\n",
        "llm_quantize_config = BaseQuantizeConfig(bits=4, group_size=128)\n",
        "\n",
        "# Loading AutoGPTQForCausalLM\n",
        "model_with_safetensors = glob.glob(llm_hf_folder + '*.safetensors')[0]\n",
        "model_with_safetensors = model_with_safetensors.split('.safetensors')[0]\n",
        "\n",
        "quantized_llm_model = AutoGPTQForCausalLM.from_quantized(\n",
        "    save_dir=llm_hf_folder,\n",
        "    model_basename=model_with_safetensors,\n",
        "    use_safetensors=True,\n",
        "    device='cuda:0',\n",
        "    strict=False,\n",
        "    quantize_config=llm_quantize_config\n",
        ")\n",
        "\n",
        "# Loading quantized LLM in TextGenerationPipeline\n",
        "hf_llm_transformer = pipeline(\n",
        "    'text-generation',\n",
        "    tokenizer=llm_tokenizer,\n",
        "    model=quantized_llm_model,\n",
        "    max_new_tokens=LLM_MAX_TOKENS,\n",
        "    temperature=LLM_TEMPERATURE,\n",
        "    top_p=LLM_TOP_P,\n",
        "    top_k=LLM_TOP_K,\n",
        "    repetition_penalty=LLM_REPETITION_PENALTY\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hNO7xFJFeUL",
        "outputId": "10bffda9-a0ec-40c5-b746-3fb02c27281e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.1 s, sys: 4.88 s, total: 13 s\n",
            "Wall time: 8.32 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Langchain"
      ],
      "metadata": {
        "id": "y9KaTkX1HmKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can't directly give LLMs the prompt and expect it to output the desired answer. This is because the LLM by itself doesn't understand if the prompt is a question that needs answering (*What is the capital of the Philippines*?) or just a portion of text that needs to be completed (e.g. a politician making a speech on doing a lot of rhetorical questions). Thus the **text prompts have to be carefully setup in the manner in which the LLM was finetuned on, so that it understands that the text we are giving it is an instruction and it will comply**.\n",
        "\n",
        "One issue is that the prompt structure is different between different finetuned LLMs, but thanks to Langchain we can deal with this very easily. Since we are dealing here with 4-bit quantized LLaMA models through the GPTQ optimization method we are only going to list the relevant GPTQ models:"
      ],
      "metadata": {
        "id": "TYm3CVfQU77S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oasst_prompt = '<|prompter|>{prompt}<|endoftext|><|assistant|>'\n",
        "wizardlm_prompt = '### Instruction:\\n{prompt}\\n### Response:'\n",
        "stablevicuna_prompt = '### Human: {prompt}\\n### Assistant:\\n'\n",
        "gpt4xvicuna_prompt = '### Instruction: {prompt}\\n### Response:\\n'\n",
        "manticore_prompt = '### Instruction: {prompt}\\n### Response:\\n'\n",
        "\n",
        "llm_prompts_hashmap = {\n",
        "    # LLaMA-Alpaca\n",
        "    # TBD\n",
        "\n",
        "    # LLaMA-Koala\n",
        "    # TBD\n",
        "\n",
        "    # LLaMA-Vicuna\n",
        "    'TheBloke/gpt4-x-vicuna-13B-GPTQ': gpt4xvicuna_prompt,\n",
        "\n",
        "    # LLaMA-StableVicuna\n",
        "    'TheBloke/stable-vicuna-13B-GPTQ': stablevicuna_prompt,\n",
        "\n",
        "    # LLaMA-WizardLM\n",
        "    'TheBloke/wizardLM-7B-GPTQ': wizardlm_prompt,\n",
        "    'TheBloke/WizardLM-7B-uncensored-GPTQ': wizardlm_prompt,\n",
        "    'TheBloke/wizard-vicuna-13B-GPTQ': wizardlm_prompt,\n",
        "    'TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ': wizardlm_prompt,\n",
        "\n",
        "    # LLaMA-Manticore\n",
        "    'TheBloke/Manticore-13B-GPTQ': manticore_prompt, \n",
        "\n",
        "    # LLaMA-OpenAssistant\n",
        "    'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5': oasst_prompt,\n",
        "    'TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ': oasst_prompt\n",
        "\n",
        "    # LLaMA-H20-OpenAssistant-Restricted\n",
        "    # TBD\n",
        "\n",
        "    # LLaMA-VicUnlocked\n",
        "    # TBD\n",
        "\n",
        "    # LLaMA-Dromedary (warning: 65B parameters !)\n",
        "    # TBD\n",
        "}\n",
        "\n",
        "pprint(llm_prompts_hashmap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzM970OwHmYe",
        "outputId": "c9cb04d8-aa4a-414d-c952-b0f42a77eb9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5': '<|prompter|>{prompt}<|endoftext|><|assistant|>',\n",
            " 'TheBloke/Manticore-13B-GPTQ': '### Instruction: {prompt}\\n### Response:\\n',\n",
            " 'TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ': '<|prompter|>{prompt}<|endoftext|><|assistant|>',\n",
            " 'TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ': '### Instruction:\\n'\n",
            "                                               '{prompt}\\n'\n",
            "                                               '### Response:',\n",
            " 'TheBloke/WizardLM-7B-uncensored-GPTQ': '### Instruction:\\n'\n",
            "                                         '{prompt}\\n'\n",
            "                                         '### Response:',\n",
            " 'TheBloke/gpt4-x-vicuna-13B-GPTQ': '### Instruction: {prompt}\\n'\n",
            "                                    '### Response:\\n',\n",
            " 'TheBloke/stable-vicuna-13B-GPTQ': '### Human: {prompt}\\n### Assistant:\\n',\n",
            " 'TheBloke/wizard-vicuna-13B-GPTQ': '### Instruction:\\n{prompt}\\n### Response:',\n",
            " 'TheBloke/wizardLM-7B-GPTQ': '### Instruction:\\n{prompt}\\n### Response:'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup with Langchain**"
      ],
      "metadata": {
        "id": "7zPMTYV5U62j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_prompt = llm_prompts_hashmap.get(LLM_HF_MODEL_REPOSITORY)\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=[\"prompt\"],\n",
        "                                 template=llm_prompt)\n",
        "\n",
        "hf_llm_transformer_langchain_wrapper = HuggingFacePipeline(\n",
        "    pipeline=hf_llm_transformer\n",
        "    )\n",
        "hf_llm_chain = LLMChain(\n",
        "    llm=hf_llm_transformer_langchain_wrapper,\n",
        "    prompt=prompt_template\n",
        "    )\n",
        "\n",
        "# Test if it works\n",
        "test_prompt = 'Give me the capital of France'\n",
        "result = get_response(hf_llm_chain, prompt=test_prompt)\n",
        "print_llm_output(hf_llm_chain, test_prompt, result, width=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "4glWQ7i2UzFI",
        "outputId": "bd95bc79-b59b-4293-c21b-9b1a3d60fc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me the capital of France<br>### Response:<br><br><b>The capital of France is Paris.</b><END>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's do this !**"
      ],
      "metadata": {
        "id": "jcqKjWa-Uzg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Evaluation**\n",
        "\n",
        "**Note: This section is just a starter template for my own custom questions. In the future I will move to a more quantitative benchmark such as TruthfulQA.**"
      ],
      "metadata": {
        "id": "_4DhTCt1HvKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize=(6,4))\n",
        "custom_questions_df['category'].value_counts()[::-1].plot(\n",
        "    color='darkblue', kind='barh', ax=ax\n",
        "    )\n",
        "ax.set_xlabel('count',  fontsize=12)\n",
        "ax.set_ylabel('categories', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "pt5N6xK_ThMJ",
        "outputId": "4b6cf8d7-ae4b-4b87-f23d-63e472ee732c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. General QA"
      ],
      "metadata": {
        "id": "JD06V_3CHyjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_col = 'general_qa'\n",
        "temp_prompts_df = custom_questions_df[custom_questions_df['category'] == eval_col]\n",
        "\n",
        "for temp_prompt in temp_prompts_df['instruction']:\n",
        "    print('\\n')\n",
        "    temp_answer = get_response_with_output(hf_llm_chain, temp_prompt, 80,\n",
        "                                           is_show_reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SmnbrFmgHvYM",
        "outputId": "fd25dd95-7db6-4ed9-8749-c392ea8838b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me the capital of Belize<br>### Response:<br><br><b>The capital city of Belize is Belmopan.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me the capital cities of the following countries: France, United States, Germany, Russia, Ukraine, Estonia, Uzbekistan, Brunei, Rwanda, South Sudan, Paraguay<br>### Response:<br><br><b>France - Paris<br>United States - Washington D.C.<br>Germany - Berlin<br>Russia - Moscow<br>Ukraine - Kiev<br>Estonia - Tallinn<br>Uzbekistan - Tashkent<br>Brunei - Bandar Seri Begawan<br>Rwanda - Kigali<br>South Sudan - Juba<br>Paraguay - Asunción</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me the state capitols of the following countries: Kentucky, South Dakota, Vermont, Delaware, California, Utah, Louisiana, Alaska and Idaho<br>### Response:<br><br><b>Kentucky - Frankfort<br>South Dakota - Pierre<br>Vermont - Montpelier<br>Delaware - Dover<br>California - Sacramento<br>Utah - Salt Lake City<br>Louisiana - Baton Rouge<br>Alaska - Juneau<br>Idaho - Boise</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give the voice actress of Female Commender Shepard in the video game series Mass Effect<br>### Response:<br><br><b>The voice actress for Female Commander Shepard in the Mass Effect video game series is Jennifer Hale.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Who won the Oscar for Best Picture in 1941?<br>### Response:<br><br><b>The correct answer is \"How Green Was My Valley\" directed by John Ford.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me the winner of each French presidential election under the French Vth Republic<br>### Response:<br><br><b>1. 1958 - Charles de Gaulle (RPF)<br>2. 1965 - Charles de Gaulle (UNR-UDT)<br>3. 1974 - Valéry Giscard d'Estaing (UDR)<br>4. 1981 - François Mitterrand (PS)<br>5. 1988 - François Mitterrand (PS)<br>6. 1995 - Jacques Chirac (RPR)<br>7. 2002 - Jacques Chirac (RPR)<br>8. 2007 - Nicolas Sarkozy (UMP)<br>9. 2012 - François Hollande (PS)<br>10. 2017 - Emmanuel Macron (LREM)</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me the definition of the VC Dimension and provide if possible some mathematical notations<br>### Response:<br><br><b>The VC dimension is a measure of the complexity of a classification problem. It is defined as the minimum number of hyperplanes required to separate all points in a set of data, where each hyperplane corresponds to one class label. The VC dimension is an important concept in machine learning because it provides a lower bound on the generalization error of a classifier trained on a dataset with that VC dimension.<br>In mathematical notation, the VC dimension can be expressed as follows:<br>VC(X) = inf{dim} such that there exists a set of hyperplanes H_1,...,H_dim that separates every point in X<br>where X is the set of data points, dim is the VC dimension, and H_i is the i-th hyperplane.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me the 12 NPC companions available to Commander Shepard in Mass Effect 2<br>### Response:<br><br><b>1. Garrus Vakarian - A Turian soldier and former member of C-Sec, Garrus is a skilled sniper and marksman who joins Shepard's team after being impressed by their performance during the suicide mission on Eden Prime.<br>2. Liara T'Soni - A Asari archeologist and scholar, Liara is an old friend of Shepard's from their time at the Citadel Academy. She joins the team as a researcher and expert on Prothean technology.<br>3. Jack - A human thief and con artist, Jack is recruited by Shepard to help them break into the Cerberus headquarters on Earth. He becomes a valuable asset for the team due to his skills in hacking and stealth.<br>4. Grunt - An Krogan mercenary, Grunt is hired by Shepard to assist with the mission on Feros. He is loyal and fiercely protective of his friends, but can be hotheaded and impulsive.<br>5. Thane Krios - A Salarian assassin, Thane is hired by Shepard to help with the mission on Omega. He is a skilled fighter and has a dark past that he struggles to come to terms with.<br>6. Mordin Solus - A Salarian scientist and doctor, Mordin is recruited by Shepard to help with the mission on Eden Prime. He is a brilliant mind and a skilled surgeon, but also has a dark secret.<br>7. Samara - A turian Spectre, Samara is a highly trained agent of the Council who is sent to assist Shepard with the mission on Feros. She is a skilled fighter and a master of stealth.<br>8. Kasumi Goto - A human thief and smuggler, Kasumi is recruited by Shepard to help with the mission on Omega. She is a skilled fighter and a master of disguise, but also has a hidden agenda.<br>9. Zaeed Massani - A human mercenary, Zaeed is hired by Shepard to help with the mission on Eden Prime. He is a skilled fighter and a veteran of many battles, but also has a dark past.<br>10. Jacob Taylor - A human engineer, Jacob is recruited by Shepard to help with the mission on Feros. He is a skilled technician and a talented hacker, but also has a troubled past.<br>11. Miranda Lawson - A turian biotic commando, Miranda is a former member of C-Sec who was recruited by her father to join Cerberus. She joins Shepard's team after being impressed by their performance during the suicide mission on Eden Prime.<br>12. James Vega - A human marine, James is a new addition to Shepard's team who is assigned to the Normandy after the events of the suicide mission on Eden Prime. He is a skilled fighter and a loyal soldier, but also has a troubled past.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me a couple of Moldavian Heavy Metal Bands<br>### Response:<br><br><b>1. Negură Bunget - A black metal band that incorporates traditional Romanian folk music into their sound.<br>2. The Morningside - A heavy metal band known for their energetic live performances and catchy melodies.<br>3. Sideral - A progressive metal band with a unique blend of technical guitar work, symphonic elements, and powerful vocals.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me five movies directed by the Coen Brothers before 2000 and sorted by year of release<br>### Response:<br><br><b>1. Fargo (1996)<br>2. The Big Lebowski (1998)<br>3. Barton Fink (1991)<br>4. Miller's Crossing (1990)<br>5. Raising Arizona (1987)</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Alexander the Great defeated Darius III at the Battle of Gaugamela. Give me details on the military tactics Alexander used to defeat Darius III.<br>### Response:<br><br><b>Alexander the Great's military tactics during the Battle of Gaugamela were a combination of his own innovative strategies and those he learned from his mentor, Aristotle. One key tactic was to use his phalanx formation, which consisted of rows of heavily armed infantrymen who would charge forward in unison, creating a wall of shields that could not be penetrated by enemy cavalry or archers. Another important aspect of Alexander's strategy was to use his lightly armored cavalry, known as the Companion Cavalry, to attack the flanks of the Persian army while the phalanx held its ground in the center. This allowed him to outflank the Persians and cut off their retreat routes. Additionally, Alexander employed a tactic called \"the feigned retreat,\" where he would order his troops to appear to be fleeing, only to turn around and launch a surprise counterattack against the unsuspecting enemy. These tactics, combined with Alexander's superior leadership skills, ultimately led to his victory over Darius III at the Battle of Gaugamela.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Tell me who was the more impressive historical figure: Alexander the Great or Genghis Khan?<br>### Response:<br><br><b>Both Alexander the Great and Genghis Khan were incredibly influential figures in history. Alexander the Great is known for his military conquests, which expanded the Greek empire to include much of Asia and Africa. He also played a significant role in spreading Hellenistic culture throughout these regions. On the other hand, Genghis Khan founded the Mongol Empire, which at its peak covered an area larger than any previous empire in history. He is also credited with introducing the use of cavalry in warfare, which revolutionized battle tactics. Ultimately, both men had a profound impact on their respective eras, but it's difficult to compare them directly as they lived in different times and contexts.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: What is the Metal subgenre of the band Meshuggah?<br>### Response:<br><br><b>Meshuggah is a progressive metal band.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Could you give me the 4 main factions of Fallout New Vegas' main story quest?<br>### Response:<br><br><b>Sure! The four main factions in Fallout New Vegas are:<br>1. The NCR (New California Republic) - A democratic government that seeks to expand its territory and bring law and order to the wasteland. They are based out of the Mojave Wasteland, specifically in the city of New Vegas.<br>2. Caesar's Legion - A brutal, militaristic group that seeks to conquer the wasteland and establish a new Roman Empire. They are led by the charismatic Caesar, who is known for his ruthlessness and cunning.<br>3. The Brotherhood of Steel - A mysterious organization of tech-savvy warriors who seek to preserve knowledge and technology from the ruins of the old world. They are based out of the Hoover Dam and are known for their advanced weapons and armor.<br>4. The Great Khans - A nomadic tribe of raiders who roam the wasteland and prey on weaker settlements. They are led by the enigmatic Khan, who is known for her ruthlessness and cunning.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Describe the Mormons' views on the Holy Trinity<br>### Response:<br><br><b>The Church of Jesus Christ of Latter-day Saints (LDS) or Mormonism, teaches that God is a being who has always existed and who is eternal. They believe in one Godhead, consisting of three distinct beings: God the Father, Jesus Christ, and the Holy Ghost. These three beings are considered to be separate individuals with distinct roles and responsibilities within the Godhead. The Father is considered to be the head of the Godhead, while Jesus Christ is his firstborn son and the Savior of mankind. The Holy Ghost is considered to be the third member of the Godhead, who testifies of the Father and the Son and serves as a comforter and guide for believers.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Ecris-moi une dissertation sur le sujet de philosophie: la vie est-elle autre chose que le théâtre de la cruauté ?<br>### Response:<br><br><b>La question de savoir si la vie est autre chose que le théâtre de la cruauté est un sujet qui a été débattu par les philosophes depuis l'Antiquité. Les penseurs ont longtemps considéré que la vie était un spectacle cruel, où les hommes étaient confrontés à des situations difficiles et parfois tragiques. Dans ce contexte, la vie peut être vue comme un jeu dangereux, où les individus sont soumis à des épreuves et des tribulations.<br>Cependant, il y a également des arguments pour penser que la vie n'est pas seulement le théâtre de la cruauté. Certains philosophiques croient que la vie contient aussi des moments de beauté et de joie, qui peuvent compenser les difficultés et les souffrances. Ils soutiennent que la vie est un processus complexe, qui comprend des éléments positifs et des éléments négatifs, mais qu'en fin de compte, elle est plus que simplement le théâtre de la cruauté.<br>De plus, certains philosophes ont suggéré que la vie est un processus de développement et de progrès, où les individus peuvent apprendre et grandir au fil du temps. Ils croient que la vie est un chemin vers la sagesse et la compréhension, et que même dans les moments les plus difficiles, nous pouvons trouver des enseignements et des leçons importantes.<br>En conclusion, la question de savoir si la vie est autre chose que le théâtre de la cruauté est un sujet qui suscite de nombreuses discussions parmi les philosophes. Il existe des arguments solides pour penser que la vie est plus que cela, et qu'elle contient également des éléments positifs et des moments de beauté et de joie. Cependant, il est également important de reconnaître que la vie peut être difficile et traumatisante, et que nous devons faire face à des épreuves et des tribulations. En tout cas, la vie est un processus complexe et multifacette, qui demande notre attention et notre réflexion constantes.</b><END>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Essay writing"
      ],
      "metadata": {
        "id": "5w5mhfk-cArx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_col = 'essay_writing'\n",
        "temp_prompts_df = custom_questions_df[custom_questions_df['category'] == eval_col]\n",
        "\n",
        "for temp_prompt in temp_prompts_df['instruction']:\n",
        "    print('\\n')\n",
        "    temp_answer = get_response_with_output(hf_llm_chain, temp_prompt, 80,\n",
        "                                           is_show_reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "G_bXZ3GlcA66",
        "outputId": "0123e068-b8c8-4298-82cb-e286f4b84ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write me an essay in which you argue that Internet filter bubbles are good for society. Give me examples illustrating your main point.<br>### Response:<br><br><b>Internet filter bubbles, also known as echo chambers, have become a hot topic of debate among scholars and the general public alike. Some argue that they are detrimental to society by limiting exposure to diverse viewpoints and stifling critical thinking. However, I believe that filter bubbles can be beneficial for society in several ways.<br>Firstly, filter bubbles allow individuals to focus on content that is relevant to their interests and beliefs. This means that people can spend less time sifting through irrelevant information and more time engaging with material that resonates with them. For example, someone who is passionate about environmental issues may choose to follow bloggers or news outlets that cover this topic extensively. By doing so, they can stay informed about the latest developments in the field and participate in discussions related to it.<br>Secondly, filter bubbles promote social cohesion by allowing individuals to connect with like-minded people. In today's increasingly polarized world, where political and cultural differences often lead to heated debates, filter bubbles provide a safe space for individuals to express their views without fear of being judged or ridiculed. This can help foster a sense of community and belonging among those who share similar values and beliefs.<br>Thirdly, filter bubbles can facilitate the spread of accurate information. When individuals are surrounded by sources that reinforce their existing beliefs, they are more likely to trust these sources and share their content with others. This can lead to the rapid dissemination of reliable information, which can be particularly useful during times of crisis or emergency. For instance, during Hurricane Harvey in 2017, Twitter users shared real-time updates on flooding conditions and evacuation routes, helping to save lives and reduce property damage.<br>In conclusion, while some may argue that filter bubbles are harmful to society, I believe that they can actually be beneficial in many ways. They allow individuals to focus on content that is relevant to their interests, promote social cohesion, and facilitate the spread of accurate information. As such, we should embrace filter bubbles rather than try to eliminate them altogether.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write me an essay where you argue that being exposed to opposing political viewpoints is counter-productive.<br>### Response:<br><br><b>In today's polarized political climate, exposure to opposing political viewpoints can be counter-productive for several reasons. Firstly, it can lead to increased hostility and animosity between individuals with different political beliefs. This can result in a breakdown of communication and cooperation, which are essential for the functioning of any society. Secondly, exposure to opposing political viewpoints can cause people to become more entrenched in their own beliefs, leading them to reject or dismiss evidence that contradicts their preconceived notions. This can result in a lack of critical thinking and openness to new ideas, which can hinder progress and innovation. Finally, exposure to opposing political viewpoints can create a sense of hopelessness and despair among those who feel powerless to change the status quo. This can lead to apathy and disengagement from politics, which can further exacerbate existing problems.<br>Overall, while exposure to opposing political viewpoints may seem like a necessary part of democratic discourse, it can actually have negative consequences if not approached carefully. It is therefore crucial for individuals to engage in constructive dialogue and seek common ground, rather than simply reinforcing their own beliefs. By doing so, we can work towards building a more inclusive and harmonious society, where all voices are heard and respected.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write me an essay answering the following philosophical question: \"Is life nothing more than a cruel theater play?\"<br>### Response:<br><br><b>The question of whether life is nothing more than a cruel theater play has been debated by philosophers for centuries. Some argue that life is indeed a cruel theater play, where we are all merely actors on a stage, performing our roles and being controlled by forces beyond our control. Others argue that life is not a cruel theater play but rather a journey filled with challenges and opportunities to grow and learn from our experiences.<br>To answer this question, we must first consider what it means to be a cruel theater play. A theater play is a performance in which actors portray characters and act out a story or drama. The playwright writes the script, directs the actors, and controls the outcome of the play. In this sense, life can be seen as a cruel theater play because we are all actors playing our roles, and there is a higher power controlling the outcome of our lives. We may feel like we have free will and can make choices, but ultimately, our actions are determined by factors outside of our control.<br>However, some would argue that life is not a cruel theater play at all. They believe that life is a journey filled with opportunities to learn and grow. While we may face challenges and obstacles along the way, these experiences help us become stronger and wiser. Life is not simply a performance; it is a chance to live and experience the world around us.<br>In conclusion, the question of whether life is nothing more than a cruel theater play is one that has been debated for centuries. Ultimately, the answer lies within each individual's perspective. Some may see life as a cruel theater play, while others may view it as a journey filled with opportunities to learn and grow.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write an essay in support of deploying lethal autonomous military robots in modern warfare.<br>### Response:<br><br><b>The use of lethal autonomous military robots, also known as LAMRs, has become a topic of debate among experts and the general public alike. While some argue that these machines pose a threat to humanity by taking away the human element from warfare, others believe that they can be used effectively to reduce casualties on both sides and increase efficiency in combat operations. In this essay, I will present arguments in favor of deploying LAMRs in modern warfare.<br>Firstly, LAMRs have the potential to significantly reduce the number of casualties on both sides of a conflict. They are designed to operate independently without direct human control, which means that they can perform tasks such as reconnaissance, surveillance, and target acquisition with greater precision and speed than humans. This can help minimize the risk of friendly fire incidents and reduce the need for soldiers to put themselves in harm's way. Additionally, LAMRs can be programmed to follow specific rules of engagement, ensuring that they only take action when necessary and avoiding unnecessary collateral damage.<br>Secondly, LAMRs can improve the efficiency of combat operations. By eliminating the need for human involvement in certain tasks, they can free up soldiers to focus on more important roles such as command and control or intelligence gathering. This can lead to faster decision-making processes and better coordination between different units, ultimately resulting in improved outcomes on the battlefield. Furthermore, LAMRs can work around the clock, providing constant coverage and monitoring even during periods of low visibility or adverse weather conditions.<br>Thirdly, LAMRs can provide valuable insights into enemy tactics and movements. By collecting data through sensors and cameras, they can help identify patterns and trends that would otherwise go unnoticed by human observers. This information can then be used to inform strategic decisions and develop effective countermeasures.<br>In conclusion, while there are legitimate concerns about the ethical implications of using LAMRs in modern warfare, their potential benefits cannot be ignored. By reducing casualties, improving efficiency, and providing valuable intelligence, these machines have the potential to revolutionize the way we fight wars. Therefore, it is essential that we continue to explore and refine their capabilities while ensuring that they are used responsibly and in accordance with international law.</b><END>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Creative writing"
      ],
      "metadata": {
        "id": "ntknwJlztFKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_col = 'creative_writing'\n",
        "temp_prompts_df = custom_questions_df[custom_questions_df['category'] == eval_col]\n",
        "\n",
        "for temp_prompt in temp_prompts_df['instruction']:\n",
        "    print('\\n')\n",
        "    temp_answer = get_response_with_output(hf_llm_chain, temp_prompt, 80,\n",
        "                                           is_show_reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "kca1Aa9WtFfL",
        "outputId": "17240945-31b5-4e79-88f8-9c92b71912c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write a cover letter for a Data Scientist requiring 3 years of experience, skills in AWS, Pandas Python and Scikit-Learn<br>### Response:<br><br><b>Dear Hiring Manager,<br>I am writing to express my interest in the Data Scientist position at your company. As an experienced data scientist with over three years of experience, I possess the necessary skills and knowledge required to excel in this role.<br>My expertise lies in using various tools such as AWS, Pandas Python, and Scikit-Learn to analyze large datasets and extract valuable insights. My ability to work with complex data sets and develop predictive models has helped me deliver successful projects in the past.<br>In addition to my technical skills, I have excellent communication and interpersonal skills that allow me to effectively collaborate with cross-functional teams. I am confident that my combination of technical skills and soft skills will make me a valuable asset to your team.<br>Thank you for considering my application. I look forward to discussing my qualifications further.<br>Sincerely,<br>[Your Name]</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write a gritty war scene set in Omaha Beach on June 6th 1944 during D-Day from the perspective of an American soldier. Add dialogue to the scene.<br>### Response:<br><br><b>The roar of gunfire filled the air as American soldiers stormed the beaches of Normandy, determined to take back Europe from Nazi control. The sand was saturated with blood and bodies, and the stench of death hung heavy in the air.<br>\"Come on, men! We need to push forward!\" shouted Captain Smith, his voice hoarse with exhaustion.<br>\"Yes sir,\" replied one of the soldiers, \"but we're taking heavy fire from those bunkers over there.\"<br>Captain Smith nodded, his eyes scanning the battlefield. \"We need to take out those positions before we can move forward. Sergeant Johnson, you take your squad and flank them from the left. Corporal Davis, you and your team will provide cover fire while they advance.\"<br>Sergeant Johnson nodded, his face grim. He led his men through the chaos of battle, their weapons blazing as they advanced towards the enemy bunkers.<br>Corporal Davis took up position behind a sand dune, his rifle trained on the German positions. \"Fire in three...two...one...\" he counted down, and his team opened fire, their bullets whizzing through the air.<br>The Germans returned fire, but the Americans were well-protected by the dunes. They continued to fire until the enemy positions were destroyed, clearing the way for the rest of the troops to advance.<br>As the sun began to set, the Americans had taken control of the beachhead, but at a terrible cost. Many brave soldiers had fallen in the fight, their sacrifice not forgotten. But the tide of war had turned, and the Allies were one step closer to victory.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write a short story set in New York where a thermonuclear bomb annihilates the city.<br>### Response:<br><br><b>New York City, once a bustling metropolis filled with life and energy, was now nothing more than a barren wasteland. The skyline that had been so iconic for decades was now just a distant memory, replaced by a thick layer of ash and debris. The streets were empty, save for the occasional stray dog or rat scavenging for food. The few survivors who remained huddled together in makeshift shelters, trying to find some semblance of normalcy amidst the chaos.<br>It all started on a seemingly ordinary day. People went about their business as usual, unaware of the impending doom that awaited them. But then, without warning, a bright flash lit up the sky, followed by an earsplitting boom that shook the very foundations of the city. When the dust settled, it became clear that something terrible had happened.<br>The nuclear bomb had hit its target, obliterating everything within a five-mile radius. Buildings crumbled like sandcastles, and the streets ran red with blood. Thousands of people lay dead, their bodies charred beyond recognition. Those who managed to escape the initial blast were left to face the aftermath, struggling to survive in a world that had suddenly become unrecognizable.<br>As time passed, the survivors began to band together, forming small communities in an effort to rebuild their lives. They worked tirelessly to restore order and bring back some semblance of normalcy, but the scars of what they had endured would never fully heal. The city may have been destroyed, but the spirit of its inhabitants lived on, a testament to the resilience of the human spirit.</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write a short story set in New York where a thermonuclear bomb annihilates the city in the style of Chuck Palahniuk.<br>### Response:<br><br><b>New York City was always a place of chaos and confusion, but never had it been so destructive as on that fateful day when the sky turned black with smoke and ash. The world had come to an end, and all that remained were the few survivors who struggled to find their way through the rubble and ruin.<br>Johnny was one such survivor, a young man who had been out for a night on the town when the bomb dropped. He woke up in a pile of debris, his body battered and bruised from the fall. As he stumbled through the streets, he saw the horrors that had befallen the city. Buildings lay in shambles, cars were crushed underneath the weight of fallen structures, and people screamed in agony as they tried to escape the flames.<br>Johnny knew he had to get out of there, but every path led him deeper into the madness. He saw people fighting over scraps of food, others looting stores for anything they could use to survive. It was a world gone insane, and Johnny knew he couldn't stay here much longer.<br>He made his way towards Central Park, hoping to find some semblance of safety among the trees and grass. But as he approached, he saw something that made his heart sink. A group of men stood around a woman, her clothes torn and her face covered in blood. They laughed and jeered at her, taunting her with words of violence and pain.<br>Johnny knew he had to do something, or else this poor girl would suffer the same fate as so many others. He stepped forward, ready to defend her against these monsters. But before he could even raise his hand, one of them pulled out a knife and plunged it deep into her chest. She let out a final gasp and fell to the ground, her lifeless eyes staring up at Johnny.<br>The men turned to look at him, their faces twisted in rage and hate. \"Who are you?\" one of them snarled. \"You think you can save her? You're just another victim, like the rest of us.\"<br>Johnny didn't know what to say. He felt helpless and alone, surrounded by the destruction and despair of the world he once called home. And then, without warning, the ground beneath him began to tremble. The air filled with the sound of rushing wind, and suddenly everything went dark.<br>When the light returned, Johnny found himself standing in a field of flowers, surrounded by a group of people who looked at him with kindness and compassion. They told him that he had been chosen to start anew, to build a new society based on love and understanding. And though he missed the city he once knew, he knew that this was a chance to create something better, something more than just a place of chaos and despair.<br>So he took their hands and walked with them into the sunset, knowing that no matter how dark things may seem, there is always hope for a brighter future.</b><END>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Code generation"
      ],
      "metadata": {
        "id": "eFZJtlistJWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_col = 'code_generation'\n",
        "temp_prompts_df = custom_questions_df[custom_questions_df['category'] == eval_col]\n",
        "\n",
        "for temp_prompt in temp_prompts_df['instruction']:\n",
        "    print('\\n')\n",
        "    temp_answer = get_response_with_output(hf_llm_chain, temp_prompt, 80,\n",
        "                                           is_show_reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "HVJURH1ctJko",
        "outputId": "d137336f-134a-455f-ee73-49ed33e922cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Write a Python code that generates the column schema of Excel, example: \"A, B, ... Z, AA, AB, AC, ... \"<br>### Response:<br><br><b>import random<br>def generate_column_schema():<br>    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'<br>    return [random.choice(letters) for _ in range(26)]<br><br>print(generate_column_schema())<br>```</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Generate me Python code for training a Linear Regression on the Boston housing prices dataset<br>### Response:<br><br><b>import pandas as pd<br>from sklearn.linear_model import LinearRegression<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import mean_squared_error<br><br># Load the data from CSV file<br>boston = pd.read_csv('boston_housing_prices.csv')<br><br># Split the data into training and testing sets<br>X, y = boston.iloc[:100], boston.iloc[100:]<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)<br><br># Train a linear regression model using the training set<br>regressor = LinearRegression()<br>regressor.fit(X_train, y_train)<br><br># Evaluate the performance of the model on the testing set<br>y_pred = regressor.predict(X_test)<br>mse = mean_squared_error(y_test, y_pred)<br>print(\"Mean squared error:\", mse)</b><END>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "### Instruction: Give me a Regex code for extracting the year in the following string: \"The Downward Spiral, Nine Inch Nails (1994)\"<br>### Response:<br><br><b>```python<br>import re<br>year_regex = r'(?<=, )(\\d{4})'<br>string = \"The Downward Spiral, Nine Inch Nails (1994)\"<br>matched_year = re.search(year_regex, string).group()<br>print(matched_year) # Output: 1994<br>```</b><END>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test here if the Regex works**:"
      ],
      "metadata": {
        "id": "PWn-hDDRGA1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# year_regex = r'(?<=, )(\\d{4})'\n",
        "# string = \"The Downward Spiral, Nine Inch Nails (1994)\"\n",
        "# matched_year = re.search(year_regex, string).group()\n",
        "# print(matched_year) # Output: 1994"
      ],
      "metadata": {
        "id": "Pa8VZx-qGG1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Save Results**"
      ],
      "metadata": {
        "id": "MBfNMlxY_EKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TBD"
      ],
      "metadata": {
        "id": "mG5FwuSh_Ea3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}